\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{biblatex}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\addbibresource{references.bib}
\linespread{1.25}
\usepackage[lmargin=1.5in, rmargin=1.5in, tmargin=1in, bmargin=1in]{geometry}


\title{Thesis Report}
\date{January 2017}
\author{Kevin Zhu}

\newlength\tindent
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}
\setlength{\parskip}{1em}

\begin{document}
\begin{titlepage}
   \centering
   {\scshape\LARGE University of Toronto \par}
   \vspace{1cm}
   {\scshape\Large Thesis Report\par}
   \vspace{1.5cm}
   {\huge\bfseries Geospatial Prediction for Goods Solicitation with Bayesian Data Fusion\par}
   \vspace{2cm}
   {\Large\itshape Kevin Zhu\par}
   \vfill
   supervised by\par
   Prof.~Scott \textsc{Sanner}
   \vfill
   {\large April 10, 2017\par}
\end{titlepage}

\newpage
\tableofcontents
\newpage

% INTRODUCTION ==================================================================
\section{Introduction}
% Context, gap and thesis purpose

% Introductory sentence
There is no single data set in the world that would allow one to get the statistical context for any kind of problem. One would usually have to retrieve information from different sources and piece them together to get a macroscopic idea of the problem space. This kind of data fusion will be explored in this thesis in the context of advertisements to solicit goods and donations. 

% Stuff about soliciting goods
Donations are a huge resource for non-government organizations for a variety of efforts. % Detail what kinds of organizations and what they do. 
Conventional strategies tend to gravitate towards a mass door-to-door solicitation of items or setups at a certain location and advertising to everyone to donate. %sentencing

The most modern methods of soliciting goods is through direct mail, door-to-door as well and online postings through Google Ads or Facebook Advertising. Physically mailing or going door-to-door have very clear drawbacks because the amount of time it requires to execute. While advertising platforms allows for wide targeting based on desired audience age, gender, estimated income level, marital and child status, the key question that emerges is how to target the demographics. 

A different approach is to observe a source that represents the availability of goods right from the beginning. Crawl community selling sites like Craigslist for latent goods availability. This solves previous concerns because it provides open and free access to data, broad coverage of goods and services and immediate availability of goods for donation. The problem that arises here is the hiding of personal details for privacy sake (and the fact that it's useless for the buyer). This presents the challenge to uncover features of the population that allow us to map between latent demographics of users and latent goods likely available for donation. 

Successful predictions of items would have great benefits for day-to-day consumers who are looking for something relatively new at an affordable price, but the real implications lie within the organization and understanding of a monolithic data source.

These efforts can be quite expensive, both in manpower and time. In order to make the most use of the non-profit organizations' resources to maximize their impacts, it becomes important to create a predictor to allow them to station their donation boxes in optimal locations based on their target effort. 

While it's obvious that soliciting baby goods from a region consisting mostly of university students is fruitless effort, this thesis aims to make predictions over a large number of item categories. 

% Talk about a data source and how it's unable to build a predictor
Local classifieds sites like Craigslist is a prime source for peer to peer selling of items. With 80 million classified ads being posted per month\cite{clfs}, the question of whether certain features that characterize the data set would allow for efficient learning of this group behaviour. 

In the Toronto region, Craigslist gets around 2500 "item for sale" posts every two hours. With each post being bucketed into 1 of 78 categories (such a furniture, baby clothing, etc), one could discover item trends and make geospatial predictions on where and when items would become available. However, the problem that arises with solely using Craigslist is that the only useful information taht can be used is the post geotag and category. With no other pieces of information regarding the poster available, one would not be able to go beyond basic clustering and heatmapping to determine category hotspots. This result wouldn't give much insight, and organizations would just be able to post up their donation boxes in the center of the hotspot -- hardly an improvement. 

A step towards better prediction would be to link different kinds of postings to different kinds of groups. In order to make this sort of connection, however, one must be able to obtain information about the people in the regions making these posts. 
% INSERT A PICTURE OF HOTSPOTS FOR DATA

% Background on machine learning capabilities. 
% Bayes and Machine learning?

With the collected data, one is able to train machine learning models on it in order to make predictions of posts in the future. 
The two types of machine learning algorithms whose usage will be explored are discriminative and generative models. 

One particular method that can be used to make predictions is a discriminative model such as Logistic Regression, as it is generally effective in the presence of an abundance of data. It can be used to learn the trends in categorical postings in Kijiji by cross referencing locations with census data in that location. On the other hand, without a sufficient or diverse training set, such models could potentially make wildly false predictions.

Another method  to querying is Bayesian Networks. By leveraging conditional probability and Bayes rule, one can create a network of conditional probabilities and make any sort of query about the data and get the associating probability. 

% Method
It is clear that data may only give information support part of the problem, therefore cross referencing and fusion with other data resources is necessary. Sources containing post information as well as locations can be fused with census data from the region of interest to generate probabilities of items becoming available in certain areas. Additional temporal information would support item predictions further into the future. While the data can be limiting, with Craigslist providing only posts and locations, and census data providing aggregate demographics of Toronto city wards, data fusion can be conducted to make inferences on an individual scale. This thesis paper will discuss a framework for Bayesian data fusion of posting data in Toronto with census data and compare it to more naive methods as well as some machine learning methods such as multi-class logistic regression. 

% WHERE TO PUT INFORMATION ABOUT AGGREGATED DATA AND DEAGGREGATION?


% END INTRODUCTION ==================================================================

% BEGIN LIT REVIEW AND BACKGROUND===========================================
\section{Background information}

\subsection{Discriminative Models}
Discriminative models are a subset of machine learning models which takes in observations of a test sample in order to place it in a certain class.
These models are trained on a wide variety of samples allow it to be able to classify test examples. The multi-class logistic regression method used in this thesis is an example of a discriminative model. 

// More background information on these models

\subsubsection{Logistic Regression}
Logistic regression uses the sigmoid function coupled with a linear function of multiple independent variables to ultimately determine a binary outcome. The sigmoid function is as follows: 
\[\frac{1}{1+e^{-z}}\]
The z is a linear function of the form
\[z = \beta_0 + \beta_1*x_1 + \beta_2*x_2 + \beta_3*x_3  + ...\]
The \(x\) values represent independent variables. The \(\beta\) values represent the weights associated with each \(x\) value. The sigmoid function can be used to calculate the probability of the outcome being 0 or 1. Given a vector of independent variables as well as the associated weights, the sigmoid function produces a value between 0 and 1 representing the probability. A threshold, usually 0.5, can be applied such that beyond the threshold, the probability maps to an output of 1 and the reverse maps to an output of 0. 

\subsubsection{Multiclass Logistic Regression}
Multiclass Logistic Regression is an extension of Logistic regression where the output can be more than just be 0 and 1. The output becomes a vector of probabilities for each possible output class. A function known as softmax, or the normalized exponential function, is used to get the probabilities of each class. The maximum probability would indicate the most likely class, and therefore that class would be the output of MCLR. The softmax function looks like the following: 
\[Pr(Y_i = K) = \frac{e^{\beta_K} \cdot X_i}{\sum_{k=1}^{K} e^{\beta_K} \cdot X_i}\]

\subsection{Generative Models}
Generative models, on the other hand, provides the probability of a test case being a result of the model, instead of putting it into one of the many cases a discriminative model would be trained on. 

<More information on BayesNets>


% Review the state of the art, what has been accomplished and what hasn't been
% Problems in current context and further detail
% This is probably 2-3 pages
% Lit Review topics
% Solicitation of stuff. Importance of donaations, etc
% Demographics predictions + verification
% Feature Extraction/Cross referencing  
% Population predictions
   
\subsection{Previous Work -- To be changed and updated}
% Transition between actual intro to this intro
% State purpose
% Establish organization
While there hasn't been much study conducted on the methods best for the solicitation of goods, there's much research on methods to help with geospatial predictions in ecological studies. This review will investigate different motivations and strategies for geospatial analysis. 

A conference presentation outlines the vast capabilities of machine learning and targets it towards geospatial prediction \cite{confpres}. Because of the vast number of variables present in geospatial problems, machine learning tends to find patterns and learn them very well. The presentation notes the possibility of training models using census data, allowing for fire risk to be predicted in Manhattan based on income, age and rent. The approach is heavily relevant to the thesis, as census data of Toronto can be used to predict goods availabilities throughout the city.

One instance of landscape pattern analysis made use of neutral geographical models cross referenced with observed landscape patterns and ecological processes \cite{lpa}. Neutral models are defined as the minimum set of rules to generate patterns without the interference of another process \cite{neutralmodel}. This method is necessarily useful for gauging the effectiveness of a predictive model because of the comparison with a reference. A model's predictive capabilities may seem very impressive, but if the neutral model were to yield close to the same results, then it would be known that the added complexity of the constructed model would not have provided a worthwhile result. 

Another paper studied the link between population dynamics with climate change. They observed population features such as population growth, migration, urbanization, aging and household composition \cite{popdyn}. The paper investigates different feature sets to aid the trend of climate change. The concept of feature extraction and investigation allows one to cross reference patterns characterized by a particular data set (climate change in this case) with a variety of feature sets. 

The final instance makes use of Markov Random Fields as well as Spatial Autoregression models \cite{autoreg}. The paper takes a different approach from the first presentation's strictly feature learning model, as it takes a time series into account. The paper compares the standard Markov Random Field model for tying in spatial information to classification problems and the Spatial Autoregression model. 

\subsubsection{Conclusion}
Each paper was crucial to the overall understanding of the manipulation of geospatial elements with data for predictive purposes. Machine learning algorithms are simple to implement on problems with geospatial context due to the abundance of features that are present. While the concept of feature extraction was prevalent in the papers, they did not seem to consider techniques like heatmapping to visualize the data. These solutions have yet to be collectively employed as a holistic predictive system that can analyze features of an area and make predictions. While machine learning algorithms can certainly provide insight to patterns in a geographical area, SAR models can possibly be more accurate since it takes time series into consideration. The comparison between simple machine learning models, SAR and the baseline model without any features would be interesting to investigate. 

   % % Background information
   % % What data is available. What are some other people doing. Understanding of the data
   % In order to solicit goods, one must understand who is willing to give away such items and at what time. Advertising to groups can be done on certain platforms like Facebook; however, they are severely limited in that they only provides tools to target certain demographics. Other platforms like Craigslist and Kijiji only provide information about item availabilities as well as location, but nothing about demographics. If one were to be limited to only one of these resources, gathering enough information to make availability predictions on a wide range of demographics would be impossible.

% END LIT REVIEW =============================================================

% Previous Material
% EXPERIMENTS

% 1. Data collection
% 2. Point out data catches
\section{Experiment methodology}

Data fusion was conducted with two sources: Craigslist and Ontario Census data. 
% Describe the kind of learning that will be done

\subsection{Data Collection}
% Information about the data
\subsubsection{Craigslist as a source for postings}
% Explanations of the data sources
The data collection would be done on a single website to eliminate the variables in inconsistency and productivity between different websites. One could gather data from Ebay and Amazon; however, the postings are mostly from larger vendors, which does not suit the needs of thesis. Craigslist and Kijiji, on the other hand, are suitable because they are well known as "local classifieds", allowing anyone to post item listings for free. Craigslist was chosen for data collection because is more widely known and also performs separation of individual sellers and retailers under the guise of individuals. 

Since Craigslist has quite a simplistic user interface, the process of data scraping is very straightforward. Furthermore, due ot its largely unchanging user interface, data collection over the course of a month would be consistent. Craigslist, however, makes a great effort to prevent users from making a large number of requests to its website, which may cause a denial of service. As a result, they IP-ban computers that make more than a certain number of requests in a set amount of time or more than a certain number of total requests in a day. The scraping effort had to be sharded onto different machines and executed using cron jobs in order to get around this issue.

Craigslist does not make its posting history public. Moreover, there is no available resource online that has collected a workable number of postings for use in this thesis. As a result, a data collection system had to be created.

The most time-efficient and workable method was to use a CL wrapper written by Julio Malegria \cite{clwrapper}, which abstracts the interface and makes automated post querying easy. A script was written to extract all the posts on the home page along with their geotags. The homepage for the "all" section contains 2500 postings at any given time. New posts bump the last post off the front page, making it invisible to the scraper. Because of the fact that it is impossible to search the entire history of Craigslist, there is the possibility that many latent variables in posting patterns would have an impact on the data gathered. In order to safely ignore the possibility that these factors exist, data was scraped from Craigslist every day at 3:00 am. 

The data collected has a number of useful attributes associated with it: the description of the item in the form of a title, geotags indicating the location at which the item is being sold, the url to the posting which contains its designated category as well as a a timestamp indicating when it was posted. The category, a three letter code, can be extracted from the URL and aids in the bucketing of posts without the need for description processing. The geotag is extremely useful, since this gives each posting an extra dimension of workable information: its geographical location. The timestamp allows for some temporal analysis. 

\subsection{Initial findings: Category distribution over Ward}
One of the first things done upon completing the data collection was a visual inspection of whether anything appeared remotely interesting. The first level of data processing was processing the scraped post data and associating each with one of the 44 wards in Toronto. Due to the highly irregular shaping of the Toronto Wards, association was done using nearest neighbours. Each posting's geotag allows for the search of the closest ward centroid. While this may not be entire accurate, it is a very cheap and efficient way of bucketing the posts. Over 55,000 posts were processed and bucketed.

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{Ward_2.png}
\caption{Categories of postings in ward 2}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=14cm]{Ward_40.png}
\caption{Categories of postings in ward 40}
\end{figure}

Graphing the distributions of categories of wards shows that in certain wards, very distinct peaks for different categories can be observed. This shows that there are features, latent or patent, that could help machine learning algorithms understand and predict posting locations. Had the resulting graphs shown largely random distributions of postings, it would not be immediately obvious that something could be learned. 

\subsubsection{Geotags} % Should just be bold, whatever
Geotags associated with the post are granular enouh such that the postings do not get geospatially bunched together. It would also be helpful describe places in Toronto beyond the most well known areas, so if the geotags are not specific enough, it would be nigh impossible to learn geospatial patterns. Categories, for example, serves as a good bucketing system for Craigslist postings. A very detailed timestamp is also required so that temporal analysis can be done is a possibility for future work and investigation.

\subsubsection{Categories}
While each posting collected has a detailed description of the item, processing this kind of information using computer models of natural language would be quite cumbersome and outside the scope of the thesis. The url associated with each piece of data contains a three letter code that, while less granular, is suitable for our purposes. The 55000 postings scraped gave rise to 78 different possible classes. 

% Picture about the classes and the number of posts 

\subsubsection{Toronto Census Data as a source for demographics}
Since Craigslist does not disclose any information regarding the person that had made a forsale posting, another data source would need to be consulted. The Ontario Census data of 2014 was taken to provide this information. Since we have knowledge of posting locations, we can cross reference the geotag with a detailed, albeit aggregated census data set. 


\subsection{Initial learning model}
Our initial predictive model was a multi-class logisitic regression model, which would be trained to learn the post categories that would be most likely to spring up from a ward. While it is not expected for the model to do better than picking the most frequent class that occurs in each ward, the model allows us to observe the ability to learn using two separate datasets.  

\subsubsection{Fusion of the data}
Data from Craigslist and Toronto census is largely useless on their own to train a model. Post data cannot be learned without knowledge about the demographics and vice versa. The piece of information relating Craigslist posts to census data is the geotag. Since census data is provided on a ward level, processing of the geotag and bucketing it into a ward would have to be done in order to bring the two sets of data onto the same playing ground. Because the wards have complex shapes, exact bucketing of the post into a specific ward by referencing borders is cumbersome. A nearest neighbours approach was taken to bucket the posts.


\subsubsection{Model Construction}
A multiclass logistic regression model is employed for initial learning. To train the model, a training set of 80\% of the gathered data points is used. The remaining 20\% of the data is used as a validation set. Each posts' category would be the output variable for the model: y. The independent variables or parameters, x, of the logistic regression function would collectively represent the distributions of features of a particular ward. With Toronto split into 44 wards, a feature set can be constructed for each.
% Using census data from Toronto, it is possible to extract features for each ward. Age, female count, male count, family size and household type 
% Having bucketed the categories into wards. While it is difficult to spot particular idiosyncrasies with certain wards, these features can be fed into a multi-class logistic regression model. 

The census data provides counts for different age groups, gender, household type and family size. These numbers can be normalized with respect to each ward to create ward profiles. The age and gender feature sets would be split into buckets, whereas household type and family size would be split into type and size buckets respectively. Once the model is trained, it will attempt to predict the post categories given a vector of parameters representing the ward profile. 

The first feature set inspected was the age distribution of wards. There are 19 age groups containing a raw count of people falling in a particular group. For learning purposes, the age groups are normalized with respect to their ward totals. This removes discrepancies between wards caused by differing populations, and thus allows for a clear age profile. The logistic regression model takes in feature vectors, which are the age groups of a particular ward. The target would be the posting that emerged from that particular ward.

% FORMULA Put in a latex formula x0 + wx1 + etc
\[w_0 + w_1 \cdot x_1 + w_2 \cdot x_2 + w_3 \cdot x_3 + ...\]

A notable caveat in this method is that, due to the fact that each ward can output posts form multiple different categories, the feature vector of age group would map to different categories. The natural consequence of this overlap is that the predicted outputs would be subject to inaccuracies. 

\subsubsection{Criteria for model accuracy}
There are three methods of gauging the model accuracy in this report: raw prediction accuracy, hit rate of 5 and log likelihood. Raw prediction accuracy is measured as 
\[\text{Accuracy} = \frac{\text{correct predictions}}{\text{total predictions}}\]

Since MCLR calculates the softmaxes for each possible class, one could count the appearance of the correct output among the top 5 of the most likely classes as a "hit". The hit rate of 5 can be calculated as follows:
\[\text{Hit Rate of 5} = \frac{\text{times category appeared in top five}}{\text{total predictions}}\]

Another method of gauging model effectiveness is through negative log likelihood. This method essentially takes the negative log of the likelihood that the MCLR model gives for the correct category, and then sums it over the entire test set. With the goal of maximizing the likelihood for correct predictions, we would expect a good model to minimize the negative log likelihood and have a total of as close to 0 as possible. This method is also useful in better comparing different feature sets. Some feature sets may have poor accuracy, but they may be very confident in the predictions which turn out to be true. This can be observed using the negative log likelihood.  

\subsubsection{Results of Multi-class Logistic Regression}

% Establish the baseline.
In order to ground the prediction accuracy of the model, a baseline must be established. Otherwise, it would be very difficult to establish just how effective a model is. 
If one were to simply guess the category of a post, there would be a 1/78 chance of getting it right. If one were to guess the most common category of a ward, some wards would give under 20\% accuracy while some wards with heavy spikes, like ward 2 seen in figures 1 and 2 would have around 50\% accuracy. 

% Put in the results of the learning here
\begin{figure}[h]
\centering
\includegraphics[width=14cm]{results.png}
\caption{Results of different feature sets.}
\end{figure}
The multiclass logistic regression is able to predict the post category with almost 20\% accuracy. This is reasonable, given the overlapping training set. 
While accuracy is important for precise predictions, it would be of great use to view the top five categories of a ward to ensure that the model is indeed learning the features that define the ward and mapping it well to the categories that the ward outputs. The hit-rate of 5 is much higher than accuracy, showing that the post category is among the top 5 most probable categories for a ward almost 50\% of the time. 

% Comparing different ward feature sets
This experiment was run using each ward's female and male counts, household type as well as family size, all normalized by the ward total. 
% Insert table about accuracy of different feature sets.
It can be seen observed that household type and family size overall is less indicative of categories than age and gender. The log likelihood for each feature set is a great way of checking the confidence of the models formed by different feature sets. It could be that despite a lower accuracy with household type and family, their correct predictions may have higher probabilities. However, observing the negative log likelihood of the predictions by household type and family size features also reveals that they are poorer predictors with higher negative log likelihoods.

% Heatmapping and visualizing the probabilities 
Another method of visualizing the raw collected data as well as the predictions is to heatmap the postings under a particular category.   
The wards are not granular enough. As a result, it is nearly impossible to create telling heatmaps when the number of samples for a particular category is under a couple hundred. 

\begin{figure}[h]
\centering
\includegraphics[width=6.3cm]{raw_clothing.png}
\includegraphics[width=6cm]{predicted_clothing.png}
\caption{Left: raw postings of the clothing category. Right: predicted "hotspots" for clothing postings.}
\end{figure}

The clothings posting had over 400 posts over the course of a month, allowing for predictions to show hot spots more prominently. While it does not completely reproduce the dense hot spots in the raw heatmap, it does manage to capture and display the densest regions for clothing postings. 

% Talk about the weights of each age group
Predicting accuracy and hit rate is useful in determining the correctness of a model; however, since each posting is made by an individual, it comes as a natural consequence to investigate the probability distribution of feature buckets (such as age or family size) for each posting. 

There are two approaches taken to investigating the predictions on a more granular level. One could use the weights of feature variables for each class as an indicator of their importance in the class' prediction. This allows one to see how much being in a particular feature bucket contributes to the final prediction. 

One could use the model to investigate the particular features that best characterize the prevalence of a category. In multi-class logistic regression, the weights give a good indicator of how important a feature is to the overall model. Features that are very close to zero are not useful in terms of contribution to the probabilities of categories, whereas large features are very indicative of category decisions. 

The following shows an example of features that play a large role in certain category determinations. 

\begin{figure}[h]
\centering
\includegraphics[width=13cm]{stuff.png}
\caption{The features that matter most for the determination of toys and games postings.}
\end{figure}

In Figure 5, the family of a single mother to a child is the most highly weighted feature. The second highest feature also has only 1 child. This could make potential sense since the child could easily outgrow his/her toys. With no child to pass it down to, it is likely to go up for sale. 
While these results are interesting, and can make rudimentary sense, there is no method of verifying the correctness of the weights. The weights get noticeably smaller as one goes down the list, and so their meanings must be considered carefully. It is possible that features subsquent to the top contender are simply noisy outputs. 

A major problem with the multi-class logistic regression model lies within the nature of its training. The model is trained with the aggregate demographics of each ward in Toronto. As a result, it is not possible to predict on an individual poster level. As a result, one would not be able to use this model to generate a probabilistic output even if poster demographics were given. 

Due to the nature of the data collected from Craigslist, there is no way to obtain the granular details of the person making a particular post. Unfortunately, without further data about the posters, this is as far as one can take with the analysis, since no verification of the results is possible. With only post categories and the geotag would be given, making it impossible to verify predictions on the type of category posted by a particular demographic. In order to address this lack of individual details, we must simulate the data points given the currently known posting productivity within the wards as well as the demographics of the ward. 

\subsection{Bayesian Inference Model}
In order to address this lack of individual details, we created a simulation of Toronto's activity on Craigslist. Since the geotag, and hence the ward, is common information between the postings and the census data, one would use the conditional probability \[P(\text{category} | \text{geotag})\] and \[P(\text{individual demographics} | \text{geotag})\] to form a simple Bayes net model. 

//Insert diagram of the Bayes net. 
// Geotag --\> Category
// Geotag --\> Individual Demographics

A ground truth Bayes net can be created knowing the relationship between different pieces of data. From the original 55000 scraped points, we have the probabilities of certain posts originating from certain wards, as this is just basic ward productivity. The simulation post first starts off with its ward origin. From here, since category is conditionally dependent on the ward (or geotag), we can generate the post category depending on where the post originates. Finally, we can generate the demographics of a person creating this post since we also have the population probability distributions in wards. This process is repeated 55000 times to generate a new simulation set with information about individual demographics.

% Using the 55000 data points scraped from Craigslist, normalized ward productivities can be created. This would allow for a "drawing samples from a bag" approach for the posts where the probability of a certain post being from a certain ward is the ward productivity over the 55000 data points. Using the ward population distributions for age extracted from census data, one could probabilistically generate the age group that the person making the post would fall in. 

% From here it becomes clear that due to the fact that the Logistic Regression model is trained on aggregate census data fused with Craigslist postings, yada unfinished

The benefit of this Bayesian model is that it basically makes individual predictions possible from aggregate data like the Toronto census, unlike the multi-class logistic regression model which can only make predictions on the aggregate data.  

\subsubsection{Predictions on the Individual}
While the logistic regression model is not supposed to able to predict the category for an individual, a hack can be done on the model to force a category out for a particular feature bucket. This is done by a one-hot encoding of the feature input (x) for the logistic regression model. Whereas in the previous experiment, a feature vector consisting of the normalized values for the probabilistic distributions of a ward would be inputted as the test example to extract the predicted category, the hack involves inputting a vector where a value of 1 corresponds with the desired bucket and 0 for everything else. This way, the model would output a category based on knowledge about one sole feature bucket. This would allow us to compare the success of Logistic Regression and the Bayes Net on a semantically equal level, 
This prediction of a category based on details of an individual essentially boils down to the query  \[P(\text{category} | \text{individual demographic})\], which can be easily found with the bayes net. 

The multi-class logistic regression model is trained on the fusion of the aggregate census data and the posting categories, unchanged from the initial prediction experiment. The prediction, however, is done by feeding a one-hot vector representing the feature bucket that the poster falls under. 


// Explanation on how to perform this query on the bayes net to be done in background or briefly here. 

The difference in results of predictions on the Bayes net versus the one-hot encoded Logistic Regression model is drastic. 

\begin{table}[]
\centering
\begin{tabular}{l|l|l|l|}
\cline{2-4}
 & Negative Log-Likelihood & Accuracy & Hit Rate of 5 \\ \hline
\multicolumn{1}{|l|}{Logistic Regression} & -47624.61 & 1.99\% & 11\% \\ \hline
\multicolumn{1}{|l|}{Bayes Net} & -24132.38 & 10.92\% & 24.6\% \\ \hline
\end{tabular}
\caption{Accuracy of category predictions on an individual.}
\label{my-label}
\end{table}

Here it becomes clear that due to the fact that the Logistic Regression model is trained on aggregate census data fused with Craigslist postings. The manipulation of the logistic regression model is not feasible, as it gives an abysmal accuracy that is barely better than blindly guessing the category. 

The Bayes model performs much better in comparison, though still not objectively well. This can be explained by the simplistic nature of the model which is composed of just 3 nodes. With more data, a more complicated model can be created, which would allow for a much more accurate prediction. 
  

\newpage
\section{Future Work: Considerations and experiments to perform}
In this thesis, the productivity of a ward is determined purely based on the number of people posting. This does not take negative samples into account; therefore, in our experiments, a ward making 200 posts an hour but with a huge population would be considered more productive that an ward making 150 posts an hour with a much smaller population. 

A much more robust data collection tool must be developed to gather accurate and up-to-date information about the Craigslist posts. Collaboration with Craigslist to attain access to the data would be greatly beneficial, as full access to data would allow for one toachieve what was previously impossible in this thesis, such as modelling the temporal characteristics of posts. 

The current bayes net that was created is very simplistic, with just 3 nodes of importance. Since the age group distributions for each ward are not extremely distinct, it becomes very difficult to each age bucket predict a very different set of top five categories to be posted. Generating a model with data from more sources would allow it to gather data  The model would be able to make more granular predictions if a greater number of data sources were used. For example, it would be greatly useful to understand more background of the people making the post, such as occupation and features of the person's surroundings. 

\section{Conclusion}
There is a huge advantage of Bayes model over the usual go-to off-the-shelf Machine Learning techniques such as logistic regression. When sensitive personal data can not be accessed, it becomes impossible for machine learning algorithms to make granular predictions on an individual. Bayes nets, on the other hand, is able to fuse and make use of the aggregate data to make predictions on a singular entity. These results have a huge impact on social service workers, who can easily make use of readily available online data to tune the targeting of their donation efforts. 

The stark contrast in predictive abilities between the discriminative methods and the generative Bayes model shows the much more principled nature of the latter model. As a result, existing data science approaches should consider the use of a more principled Bayesian approach to data fusion, as the conventional off-the-shelf classifiers and scikit-learn toolboxes may not be enough or effective for the task. 

% PREVIOUS STUFF=====================================================================================================

   % % Simulations 
   % The greatest challenge with the thesis does not lie within the construction of a model that can accurately predict posting categories, but rather the ability to verify the correctness of such a model. Currently, there is no efficient way of verifying whether such results are indeed representative of the population.

   % From the data gathered over Craigslist, we can calculate the following:
   % \[P(\text{category} | \text{demographics})\]
   % % P(category | demographics)  

   % In order to verify whether the predictions made are reputable, one could distribute surveys among the different wards in Toronto to construct the demographic profiles that are predicted by the multi-class logistic regression model. The however, would create great sample bias since individuals would largely choose to maintain their privacy. Furthermore, the possibility of false survey results remains an issue.

   % In order to have full visibility of the demographic numbers, it is necessary to create a contained system modeling Toronto along with its wards and ward population behaviours. This way, one would be abel to observe the population distributions among a ward as well as each individuals. As a result, predictions about categories tied to age group can be confirmed by observing the actual preferences of age groups through synthetically created raw data. This would be the first course of action in the coming months. Afterwards, more time would be spent on creating an SAR model to incorporate time elements into the geospatial predictions. This model could be compared with the simple MCLR model in terms of accuracy. Further heatmaps will be created to make it easier to visualize the comparisons. 

   % While the main project of simulating Toronto as well as creating a new model are underway, the MCLR model will continue to be tweaked with varying combinations of feature sets (combining male and female ward profiles for example). Furthermore, a new baseline model where no features are fed into it will be created to better quantify the role of different features in category predictions in a ward. 

   % \section{Conclusion}

   % Much of the analysis infrastructure for Toronto has been constructed up to this point. Preliminary model results are able to corroborate with prior stereotypical interpretations. Subsequent work will be done to improve the current MCLR model as well as explore additional models like SAR to process the data entirely differently. Because of the extreme difficulty of confirming the results of the models, it is necessary to create a simulated Toronto in order to cross check results with the actual population information, which is unavailable in reality. 
% END OF PREVIOUS STUFF=====================================================================================================


\newpage
\printbibliography

\end{document}
